{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inline plotting\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in training sample files \n",
    "import csv\n",
    "bbs = pd.read_csv('bbs-train.txt')\n",
    "imgs = pd.read_csv('imgs-train.txt')\n",
    "label = pd.read_csv('label-train.txt')\n",
    "list = pd.read_csv('list-train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4546, 721)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import *\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#dataframe\n",
    "df_imgs = np.loadtxt('imgs-train.txt', delimiter = ' ')\n",
    "df_label = np.loadtxt('label-train.txt')\n",
    "df_label = df_label[:,1]\n",
    "df_imgs = df_imgs.astype(np.float32)\n",
    "df_label = df_label.astype(np.float32)\n",
    "df_imgs = pd.DataFrame(df_imgs)\n",
    "df_imgs\n",
    "df_label = pd.DataFrame(df_label)\n",
    "df_label\n",
    "df = pd.concat([df_imgs, df_label], axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dimensionality Reduction Using Principle Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.mlab import PCA\n",
    "\n",
    "data = np.array(np.random.randint(10,size=(10,3)))\n",
    "results = PCA(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "df_label = stdsc.fit_transform(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_label = pca.fit_transform(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Principal Components:  Cumulative Explained Variance Ratio')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAENCAYAAABjBEDGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XdUFOfjNfArKM0SUdRoYvwqZBGV\nJiqCWDFqEIIFjQ2NCIqJPSCosRKsEXvvogYLRCxorNEk9l5RmkJsiMTOLu15//BlfqwUQReEyf2c\nwznZmdln7zw7u3eZHUwZIYQAERGRTGh97ABERESaxGIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYi\nIpKVdxabu7s7TE1NpZ/69evD0tISrq6u2L59e773DQsLg6mpKVQqlUbCnj59GqampoiJidHIeACw\naNEitGjR4p3bXbhwASNHjoSDgwMsLCzQsWNHBAUF4dmzZxrLIhcXL17E6dOni/xxMjIysHnzZvTs\n2RNNmjRB06ZN0bt3b0RERBT5YxdEQY+t7I4dO4bIyEjptqmpKX799VdNR5NkvUbz+vH399fYY7Vr\n1w6//PJLgbd3d3fH6NGjNfb4efH390fPnj1zXbd48WKYmZnh4cOHua6/ffs2TE1NsX///vd+/Hnz\n5qFVq1bvff/isn379hzHR/369WFjY4O+ffvi7NmzhRrv9evXCA4OzjF+enr6B2ctW5CNHB0dMXXq\nVOl2SkoKQkND8dNPP6FSpUro2LFjrvdzcnJCy5Ytoaur+8FBAcDa2hp//fUXqlSpopHxCmrz5s2Y\nPn06+vTpg6VLl8LQ0BCRkZGYM2cO/vjjDwQHB+OTTz4p1kwlWa9evTBlyhTY2toW2WOkpqZi0KBB\niI+Px7Bhw9C4cWMIIfD777/Dx8cHt2/fxqhRo4rs8YvCvXv3MHjwYKxatQr169cHAPz111+oWLFi\nkT/2kSNHoKOjk2O5np5ekT92XhYtWgRtbe2P9vgA4ObmhqVLl2L37t3w8vLKsT4sLAxVq1aFo6Pj\nez/G4MGDMWDAgA+JWayOHz8OLa03vxNlZmYiISEBQUFBGDx4MPbv348aNWoUaJyVK1ciLCwM7u7u\nAAAXFxe0adMGZcsWqJbyVaARdHV1Ua1aNbVlo0ePxv79+7Fr1648i01PT0+jLwwdHZ0cOYpaZGQk\npk+fDl9fX3z33XfS8tq1a6N+/fr4+uuvsXbt2mL5ZEn/Z8GCBbh+/Tp2796Nzz77TFpuYmICLS0t\nLFy4EC4uLjA2Nv6IKQsnt38robiOdyMjI419ANWUypUrf+wI+PTTT9GyZUvs2rUrR7Glp6dj165d\n6Nq1K8qVK/fej1G+fHmUL1/+Q6MWm6pVq6qVT40aNTBr1iy0b98ehw8fRp8+fQo0ztvHuyb74oO+\nY9PW1pZeDP7+/vj+++8xZMgQWFtbY+bMmTlORbZr1w4rV67E6NGj0bhxYzg4OMDf3x+vX7+Wxvz3\n338xYcIE2NnZwdraGu7u7rh27RqAnKci3d3d8fPPP2PcuHGwtraGvb09Zs2ahdTUVGm8CxcuYODA\ngWjSpAkaNWoER0dHrF69usD7uH37dlSsWBH9+vXLsa527drYsGGD2rqLFy9Kj9ekSROMGDEC9+7d\nk9a7u7tj9uzZmDp1KmxsbGBra4v58+cjPj4eHh4esLS0RLt27bBr1y61+7xrPx88eAA/Pz/pVGmf\nPn3UTg0sWrQIvXv3xvr169G2bVtYW1ujd+/euHTpkto+/fbbb+jcuTPMzc3RsWNHLF68WO1xTE1N\nsXXrVnh6esLS0hJt27bFjBkzpNMHpqamAIApU6ZIn8SuXr0Kd3d3WFtbo3HjxvDy8kJ0dLQ0pr+/\nP9q1a1fg5yQtLQ07duyAm5ubWqll6d+/PzZs2IDPP/9cmr+3P3hkP/30zz//wNTUFEeOHEHPnj1h\nbm4OJycnnDt3Drt27cJXX30FS0tLuLu7IyEhQe0+x48fVxs3v1OHMTEx+OGHH2Bra4uGDRuiVatW\nmD17NjIyMvDPP/9In/q9vLykU4BZ4z148ABmZmY4ePCg2phLly6Fg4MDMjIyALz7+XtfM2bMgIWF\nhfTay8jIQK9eveDq6gqVSoVFixahZ8+eWLNmDezt7dG4cWMMGzYMjx49ynPMsLAwdO3aFZaWljA3\nN0e3bt1w7NgxaX325y3rtX/8+HG4urrCwsICzs7O2LFjh9qYFy9ehLu7OywsLNCyZUv4+fkhKSlJ\nWp+WlobZs2ejRYsWsLa2xsSJE9/5VUmPHj1w+/ZttVPEwJvTxsnJyWqnMXfs2IEuXbrA0tISFhYW\n6NatG/78809pfe/evTFhwgT06tULNjY2CA4OznEqMioqCkOHDpWOk9atW+OXX35BZmYmgDfvSe3a\ntUN4eDg6duwIS0vLHHMHABEREejWrRssLS3Rpk0bLFy4UBojK6uTk5N0rCxduhRpaWn5zkVesn7b\nz154+c3FvHnzsHz5cjx69AimpqY4d+5cjlORKSkpmD9/Ptq3by+9Jt/19VeW9yq2ly9fYsWKFYiJ\niYGTk5O0/PDhwzA3N0d4eDj69u2b630XLVqERo0aISwsDD/++CP27NmDtWvXAnjzYhk0aBCuXLmC\nefPm4bfffkOtWrUwcOBAJCcn5zpeSEgIMjMzsX37dkyZMgVhYWGYNm0aACAxMRGDBg1C3bp1sX37\nduzevRudOnXCnDlzcPny5QLt67Vr12BhYZHnr8c2NjbSp+orV67A3d0dNWvWxJYtW7Bq1SokJiai\nb9++eP78uXSfjRs3okqVKti5cyf69++PZcuWYcCAAejevTvCwsJgbW2NCRMm4OnTpwXaz5cvX6J3\n796Ii4vDwoULERoaCmNjYwwcOFCtuK5evYrjx49j8eLFWL9+PVJSUuDr6ysd7Fu3bkVAQAC8vLwQ\nERGB8ePHY+fOnTm+Z5k1axa+/vprhIeHo1+/fli/fj327NkD4M2pMwDw8fHBokWLkJmZiaFDh+KL\nL77Azp07ERISgvT0dPzwww/SeBMmTMjxBpWfhIQEPH36FNbW1rmuL1++PJo1a1bo30ACAgIwYsQI\nhIeHo3z58vj+++/x66+/IigoCGvXrkVcXFyhviPKTqlU4rvvvoO2tjaCg4Oxb98+eHh4YM2aNfj9\n999Rs2ZN6UU7d+5cTJgwQe3+NWvWhL29PcLDw9WWh4eHo0uXLtDW1i7w8/c+fvzxR/zvf//DuHHj\nkJGRgcWLF+PWrVuYP3++NM83btzA3r17sWzZMqxduxb37t2Dh4dHrm+Whw8fxsSJE9G7d2/s3bsX\nISEhqFKlCsaOHQulUplnjhkzZsDX1xdhYWEwMzPDpEmTpA8bkZGRGDBgAGxsbLBz504sWbIEjx8/\nRq9evZCSkgIACAwMRFhYGKZMmYIdO3ZAR0fnnd/JtmnTBtWqVcsx92FhYbC1tUWdOnUAAAcOHMDk\nyZPRt29f7N27F1u2bIGhoSF8fX3VPlyEhoaiZ8+e2LZtGzp16qQ2ZkpKCgYOHAg9PT1s2rQJ+/bt\nw4ABA7Bq1Sq1DzWJiYkIDg7GjBkzEBISgkqVKsHX11f6JWH//v0YM2YMOnTogJ07d2Ly5MkIDg7G\nkiVLALz5emXmzJkYMmQIIiIi4O/vj9DQUIwfPz7fucjNo0ePEBAQgAoVKqBNmzYFmous06/VqlXD\nX3/9BUtLyxzjjho1CqGhofD398fu3bvRs2dPTJ06FRs2bHh3KPEO/fr1Ew0aNBBWVlbCyspKWFhY\nCIVCIVq0aCHWrl0rbefn5yesrKxEZmamtCw0NFQoFAqhVCqFEEK0bdtWDBo0SG38QYMGCQ8PDyGE\nEH/99ZdQKBTi5s2b0vqUlBQRGBgooqOjxalTp4RCoRDR0dFStg4dOoj09HRp+02bNokGDRqIp0+f\nivj4eLFixQqRlpYmrVepVEKhUIitW7cKIYRYuHChsLe3z3P/O3ToIH788cd3TZMQQoiRI0eKzp07\ni4yMDGlZYmKiMDc3F+vWrZMyd+7cWVr/6tUroVAoRGBgoLTs2rVrQqFQiIsXLxZoPzdv3iwaNmwo\nHj58KK3PzMwUXbt2Fd7e3tJ+KhQKkZiYKG2zd+9eoVAoxKNHj4QQQrRq1UosWbJEbZ/++OMPoVAo\nREJCghBCCIVCISZNmqS2TadOncTEiROl2wqFQmzZskUIIcTTp0+FqampmDNnjkhNTRVCCPHw4UNx\n+vRptXkqjAsXLgiFQiH+/vvvAm3fr18/MWrUKLVlfn5+okePHkIIIRISEoRCoRCrV6+W1m/atCnH\nsRgQECA6duyodp9jx46pjZt937MfW0+ePBErV64UycnJats7ODiIoKCgPMfMPt7evXtFw4YNxdOn\nT4UQQly8eFEoFAoRExMjhCjY8/e2rNdo1uv77Z/r169L296+fVuYm5sLPz8/YWZmJnbu3CmtW7hw\noWjQoIGIj4+XlkVFRQmFQiEOHz4shHjz+p8zZ44QQoizZ8+K0NBQtSxZr/+4uDghhPrzlvXa37t3\nr7T948ePhUKhELt37xZCCOHj4yMGDBigNuaLFy9Ew4YNRVhYmPTfwcHB0vrMzEzh4uIiHQt5mTt3\nrnBwcJCO2SdPnoiGDRuq5Tlz5owICwtTu1/W/GfNS69evYSzs7PaNkFBQaJly5ZCCCGSkpLEihUr\npOc4i52dnViwYIEQQoht27YJhUIhrl69Kq0/d+6cUCgU4vLly0IIIdzc3MTw4cPVxti7d6/YtGmT\nEEKIFi1aiBUrVqitP3z4sFAoFOLBgwe5zkHW42Y/Pho1aiTMzc2Fh4eH2mulIHORfb+zj5+WliYi\nIyOFQqEQv//+u9oY06ZNE7a2tu987yjQd2wODg5Sk2tpaaF8+fK5XsBRu3ZtlClTJt+x6tWrp3a7\nYsWK0qmCyMhI6OnpSV+cA2/Ou2Y9dvZTClmaNWum9gWzjY0N0tPTERUVhSZNmqBnz54ICQnB7du3\nER8fj1u3bgGAdOrmXapUqaL2m1N+bt26haZNm0pfrAJvviOpW7eu2mmMunXrSv9tYGAAANKnPuD/\nvrDPfookv/28desWPv/8c7UvbcuUKQMbGxscPnxYWlapUiW172wqVKgA4M3pmeTkZDx8+BDLli3D\nqlWrpG3E/z8PHhMTI53ay+05zOsUxieffIIhQ4ZgxYoV2LJlC5o1a4aWLVvim2++UZunwsg69gr6\nvBRU9udFX18fQM7n5X1P61WpUgV9+/ZFREQEbty4IR2LiYmJBT4W27dvj/Lly2Pfvn3o1asXdu7c\nicaNG6NevXqFev5ys3379ly/J6pZs6b0319++SVGjx6NmTNnwsnJCa6urmrbfv7556hdu7Z028TE\nBJUqVUJkZGSOU81NmjRB1apVsWzZMsTFxSE+Pl56jeQ3H9mPvayLarKOvRs3buDOnTs5fpNPT09H\ndHQ0TExMkJaWBgsLC2ldmTJlYG1tjZs3b+b5mMCb05ErV67EqVOnYG9vj127dqFChQpo3769tE3T\npk1RtWpVLF26FHfu3FHbp+ynAP/3v//l+ThVq1bN9Th58uRJjnnJby5u3bqFr7/+Wm37rLNriYmJ\nePz4MRYtWoRly5ZJ67MfK59++mmeGUNDQ6GtrY2nT59iyZIliImJwahRo9Tetws6F3nJep9u0qSJ\n2vKmTZti06ZNSExMzDdjgYrNwMBA7QWel4J88ZfblVdZ3ucL2LdPEWY9+dra2oiJiUHfvn1hYmKC\nli1bonXr1mjUqFGhLq21trZGaGgoMjIycr1Ca86cOcjMzISfn1+eY2RmZqrtd26nNd/1Jp/ffuZF\nCKE2p3nNvRBCOth8fHyk0wnZZS/E3MYR+fxPIkaPHo0+ffrg2LFjOHXqFObNm4d169YhJCQERkZG\ned4vL7Vr14aRkREuXryodio8y8uXL/H999/D09Mzz+c6tyIuzPOS2we4/L6fePz4MXr37o2KFSvC\n0dERzZo1g4WFBXr37p3nfd6mo6MDFxcXhIeHo1u3bti3bx98fX0BoFDPX25q165doFO3V69eRdmy\nZXHp0iU8f/4clSpVktblNn+ZmZm5HqMRERHw9fVFp06dYGFhga5du+L169f4/vvv83383N4jso69\nzMxMdOrUCSNGjMixTcWKFaXvut8+VgvyvlO7dm3Y2toiPDwc9vb2+O2339C1a1e118KuXbswbtw4\n6Xurbt264fnz5xg+fLjaWPnN86NHj9C7d29UrlwZ7dq1Q/PmzWFhYYEePXrk2Da/12F+VxZmbePv\n7w8HB4cc66tXr57nfQHgiy++QNmyZVGnTh0sWbIE/fv3x6BBgxAaGip9sCnoXBRW1nGeX48AJewP\ntE1MTKBUKqW2Bt68WbRt21btYors3v6u7MKFC9DV1cWXX36JX3/9FeXLl0dwcDCGDBkCR0dH/Pvv\nvwDyfyPOrnv37nj16pXa31tkuXPnDjZv3iy9cE1NTXHhwgW1TySJiYm4c+cOTExMCvR4eclvP01N\nTfHPP/+o/a2NEALnzp3Dl19+WaDxq1atiqpVqyI+Ph516tSRfu7fv4/Zs2erXeBTGA8ePMCUKVNQ\npkwZ9OzZE0FBQQgLC0NCQgLOnDnzXmNqaWnBzc0NoaGhePDgQY71mzZtwunTp6ULS3R0dPDy5Uu1\nbeLj49/rsbNkvRlmHze/Mffs2YPExESEhIRg2LBh6NSpE/T19fHkyRPpWHzX2Q7gzeXnFy9eRGho\nKFJTU6VP5UX1/GW3c+dO7Nu3DytXroSWlhamTJmitj4hIUHtu/Bbt27h5cuXMDc3zzHWsmXL4OLi\ngrlz52LAgAGws7NTu8jqfSgUCkRFRaF27drS/lesWBGBgYGIioqCsbExdHV1ce7cObX7Xb16tUDj\n9+zZEwcPHsT169cRGRmZo2yWL1+OLl26YM6cOejfvz+aN2+eZ5nmZdeuXUhOTpaOk44dO0JXVzfP\nawzyYmJigitXrqgtW7NmDbp164Zq1aqhcuXKuHv3rtqxkpCQgNmzZ0vfRxZEuXLlMGfOHKSnp8PP\nz0/az4LMRX7He9ZFaG8/V2fPnoWRkREMDQ3zzVWiis3Ozg6Wlpbw9/fH+fPnERsbiwkTJkCpVMLe\n3j7X+1y/fh2zZs1CXFwc9u3bh0WLFqFfv36oUKECatasicePH+Pw4cO4f/8+jh07Jl1lVdBTSsbG\nxhgzZgxmzZqF6dOn4/r164iPj0d4eDgGDhyIevXqwdvbGwDg4eGBO3fu4KeffkJUVBQuXbqEESNG\noHLlyvjmm28+aG7y289vvvkGRkZGGDlyJC5cuIDo6GhMmjQJt2/fxsCBAws0fpkyZTB48GCEhIRg\n7dq1uHv3Lv7880/4+/sjJSWlUL9ZlS9fHjExMXjy5AmqVq2KQ4cOYeLEiYiMjER8fDy2bt2KcuXK\noVGjRgCAFy9eFPqFO3ToUJiYmKBXr14ICwvD3bt3cfPmTcyZMwcLFizAqFGjpEv9ra2tcfr0aRw8\neBAJCQlYuHAhbt++XajHe1u1atWkq2Jv376Nq1evYuLEiXl+kqxZsyZUKhV2796N+/fv48yZMxg6\ndCjS0tKkYzHrku/bt29LH8DeVr9+fTRo0AC//PILOnXqJN3nQ5+/pKQkPH78OMdP1vOSkJCAgIAA\nDBw4EC1atEBgYCAiIiKwc+dOaQyVSgVfX19ERkbi/Pnz8PX1hZWVFZo3b57j8WrVqoVLly7hypUr\nSEhIwLZt27BgwQIABX9tvm3QoEHSe8bt27dx/fp1jBw5Ejdu3IBCoYCBgQH69+8v/V1a1sVABS22\nr776CuXKlcPkyZPRrFmzHKfka9asiYsXL0r7FBISgoULFxZqn2rVqgWlUom9e/fi3r17OH36NLy9\nvZGRkVGoeRkyZAh+//136Vg4dOgQli9fDkdHR2hpacHLywtbtmzB+vXrcffuXRw/fhzjxo1Dampq\nof9O+PPPP8ePP/6I8+fPY8uWLQWei/Lly+PFixeIiYnJcWWqqakp2rRpg4CAABw6dAh37tzB+vXr\nsX37dgwaNOidHwI//C/hNKhMmTJYsmQJZs6cKT2ZVlZWWLduHYyMjHL9F0dat26NpKQkdOnSBZUr\nV8bAgQMxZMgQAG8uF7579y5++uknKJVKfP755/j2229x6NChHJ9m8uPh4QFjY2Ns3LgRXl5eePny\nJT777DO4urpi0KBB0ndVFhYWWLt2LebPn4/u3btDX18f9vb2mDt37gf/TU5++1mhQgVs3rwZs2bN\nwuDBg5Geng5zc3OsX78eNjY2BX6M7777Dvr6+ti4cSOCgoJQuXJldOjQAWPGjClU1sGDB2PFihU4\nc+YMdu3ahTVr1mD27Nno378/lEolzMzMsHLlSnzxxRcA3lypdubMGRw5cqTAj6Gnp4eNGzdi/fr1\nWLduHX7++WeULVsWX375JRYsWIAOHTqo7VdCQgL8/f1RpkwZfP311/Dw8MDff/9dqP3KrkyZMpgz\nZw4CAwPRrVs31KpVC8OGDcuzoDt27Ahvb2/Mnz8f06ZNQ82aNeHk5ITPPvtMOhYrV66M3r17Y8GC\nBTh37hyWL1+e61hubm6YOnUqunfvrrb8Q56/vP7cokaNGjh69Ch8fX1Ro0YN6Y/emzdvjt69e2Pa\ntGnSMVa1alU0a9YMAwYMQGZmJtq3bw9/f/9cT+dOnDgRkydPxnfffYeyZcvC2NgYM2fOhI+PDy5f\nvgwzM7N3Zn5b1utv4cKF6NGjB3R1dWFjY4ONGzdKn/DHjBkDfX19zJkzB0+fPkXr1q3h5uaW41L+\n3Ojo6KBLly5Yv359rlfHTpkyBVOmTMGAAQNQrlw5mJiYYPbs2RgzZgwuX74MhULxzsdwcnLCzZs3\nMXfuXDx//hw1a9aEi4sLatWqVeAruYE3/6jG9OnTsXr1agQFBaFGjRrw9PTE4MGDAQCenp4wMDDA\npk2b8Msvv6BKlSrv9VrP0qdPH+zbtw9z585F27ZtCzQXTk5O2LlzJ1xdXREUFJRjzPnz52PevHmY\nMmUKnj59irp162Ly5Mm5npZ9WxlR0N+RSyB3d3cYGRlh3rx5HztKkfqv7CeVXosWLUJISMgHfVgg\n0pQSdSqSiIjoQ7HYiIhIVkr1qUgiIqK38Tc2IiKSlRJ1VaQmnD9//mNHICIqdQpzBXVJJ7tiA4CG\nDRt+1P+P1LsolUpcv369ROcsDRkB5tQ05tSs0pAzK6Oc8FQkERHJCouNiIhkhcVGRESywmIjIiJZ\nYbEREZGssNiIiEhWWGxERCQrLDYiIpIVFhsREckKi42IiGSFxUZERLLCYiMiIllhsRERkayw2IiI\nSFZYbEREJCssNiIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGxERGRrLDYiIhIVlhsREQkKyw2\nIiKSlSIptitXrsDBwSHP9Xv27IGjoyOsra0xZMgQJCUl5dgmOjoaFhYWuH37dlFEJCIimdJosQkh\nsGPHDnh4eCAtLS3XbSIjIzF58mQEBQXh5MmTMDIywtSpU9W2SUtLw9ixY6FSqTQZj4iI/gM0WmzL\nly/Hxo0b4e3tnec2u3fvhqOjIywtLaGnpwcfHx8cPnwYT548kbZZsGAB7OzsNBmNiIj+I8pqcrDu\n3bvD29sbZ86cyXOb2NhYWFtbS7cNDQ1RsWJFxMbGomrVqjh37hz++usvbNu2DatXr36vHCX9N72s\nfCU5Z2nICDCnpjGnZpWGnCU52/vSaLFVr179ndukpKRAT09PbZm+vj5SUlLw8uVLTJgwAQsWLICO\njs5754iOjn7v+xan0pCzNGQEmFPTmFOzSktOudBosRWEnp4elEql2rKUlBQYGBggICAA3bp1Q/36\n9T/oMUxMTKCrq/tBYxQllUqF6OjoEp2zNGQEmFPTmFOzSkPOrIxyUuzFZmxsjLi4OOl2cnIynj17\nBmNjY+zbtw86OjpYtWqVtL5Xr16YOnUqXFxcCvwYurq6OX4rLIlKQ87SkBFgTk1jTs0qLTnlotiL\nzdnZGf369UP37t1hbm6OoKAgtGrVCoaGhrhy5YratqampggJCYFCoSjumEREVEoVS7FNmjQJADBt\n2jSYmZkhICAAEyZMwOPHj9GkSRPMmDGjOGIQEdF/QJEUm62tLU6fPi3dnjZtmtp6JycnODk5vXOc\nW7duaTwbERHJG/9JLSIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGxERGRrLDYiIhIVlhsREQk\nKyw2IiKSFRYbERHJCouNiIhkhcVGRESywmIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYiIpIVFhsR\nEckKi42IiGSFxUZERLLCYiMiIllhsRERkayw2IiISFZYbEREJCssNiIikhUWGxERyQqLjYiIZIXF\nRkREssJiIyIiWWGxERGRrBRJsV25cgUODg55rt+zZw8cHR1hbW2NIUOGICkpSVp36NAhuLi4oHHj\nxujcuTMOHjxYFBGJiEimNFpsQgjs2LEDHh4eSEtLy3WbyMhITJ48GUFBQTh58iSMjIwwdepUAEBc\nXBzGjh2L8ePH4/z58xg3bhzGjh2LmJgYTcYkIiIZK6vJwZYvX459+/bB29sbq1atynWb3bt3w9HR\nEZaWlgAAHx8ftGjRAk+ePMG9e/fQs2dP2NnZAQAcHBxQt25dXL16FcbGxgXOoVKpPnxnilBWvpKc\nszRkBJhT05hTs0pDzpKc7X1ptNi6d+8Ob29vnDlzJs9tYmNjYW1tLd02NDRExYoVERsbCwcHB7VT\nmAkJCYiKikL9+vULlSM6Orrw4T+C0pCzNGQEmFPTmFOzSktOudBosVWvXv2d26SkpEBPT09tmb6+\nPlJSUtSWPXr0CF5eXujatWuhi83ExAS6urqFuk9xUqlUiI6OLtE5S0NGgDk1jTk1qzTkzMooJxot\ntoLQ09ODUqlUW5aSkgIDAwPp9o0bN+Dt7Y02bdpgypQphX4MXV3dHOVZEpWGnKUhI8CcmsacmlVa\ncspFsV/ub2xsjLi4OOl2cnIynj17Jn2Hdvz4cbi7u+O7777DtGnToKXFv0ggIqKCK/bWcHZ2xoED\nB3Du3DmoVCoEBQWhVatWMDT/HbFsAAAXWElEQVQ0RFRUFEaMGIFp06bBw8OjuKMREZEMFEuxTZo0\nCZMmTQIAmJmZISAgABMmTICdnR0SExMxY8YMAMDGjRuhVCrx008/wdraWvrZunVrccQkIiIZKJLv\n2GxtbXH69Gnp9rRp09TWOzk5wcnJKcf9AgICEBAQUBSRiIjoP4JfYBERkayw2IiISFZYbEREJCss\nNiIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGxERGRrLDYiIhIVlhsREQkKyw2IiKSFRYbERHJ\nCouNiIhkhcVGRESywmIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYiIpIVFhsREckKi42IiGSFxUZE\nRLLCYiMiIllhsRERkayw2IiISFZYbEREJCssNiIikhUWGxERyUqRFNuVK1fg4OCQ5/o9e/bA0dER\n1tbWGDJkCJKSkqR1J06cgLOzM6ysrNCnTx/ExcUVRUQiIpIpjRabEAI7duyAh4cH0tLSct0mMjIS\nkydPRlBQEE6ePAkjIyNMnToVAJCUlIRhw4ZhzJgxOHPmDOzt7fHjjz9qMiIREcmcRott+fLl2Lhx\nI7y9vfPcZvfu3XB0dISlpSX09PTg4+ODw4cP48mTJzhw4ADMzMzQrl076OjoYOjQoUhISMC1a9c0\nGZOIiGSsrCYH6969O7y9vXHmzJk8t4mNjYW1tbV029DQEBUrVkRsbCxiY2NhbGwsrdPW1kbt2rUR\nHR2NRo0aFTiHSqV6vx0oJln5SnLO0pARYE5NY07NKg05S3K296XRYqtevfo7t0lJSYGenp7aMn19\nfaSkpCAlJQUVKlTIdV1hREdHF2r7j6U05CwNGQHm1DTm1KzSklMuNFpsBaGnpwelUqm2LCUlBQYG\nBtDX189zXWGYmJhAV1f3g7MWFZVKhejo6BKdszRkBJhT05hTs0pDzqyMclLsxWZsbKx2pWNycjKe\nPXsGY2Nj1KtXD/v375fWZWRkID4+HiYmJoV6DF1d3Ry/FZZEpSFnacgIMKemMadmlZacclHsf8fm\n7OyMAwcO4Ny5c1CpVAgKCkKrVq1gaGiIr776CteuXcOBAweQmpqKZcuW4dNPP0WDBg2KOyYREZVS\nxVJskyZNwqRJkwAAZmZmCAgIwIQJE2BnZ4fExETMmDEDAFCtWjUsXboUixcvhq2tLU6cOIFFixah\nTJkyxRGTiIhkoEhORdra2uL06dPS7WnTpqmtd3JygpOTU673bd68OXbt2lUUsYiI6D+A/6QWERHJ\nCouNiIhkhcVGRESywmIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYiIpIVFhsREckKi42IiGSFxUZE\nRLLCYiMiIllhsRERkayw2IiISFZYbEREJCssNiIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGx\nERGRrLDYiIhIVlhsREQkKyw2IiKSFRYbERHJCouNiIhkhcVGRESywmIjIiJZYbEREZGssNiIiEhW\nNFpsN27cgJubG6ysrODq6opLly7l2CY9PR3z5s1Dy5YtYWtriwkTJuDVq1fS+sOHD8PJyQmNGzeG\nq6sr/v77b01GJCIimdNYsalUKnh7e6Nbt244e/Ys3N3dMWzYMKSmpqptt27dOuzevRvr16/HsWPH\nkJmZifHjxwMAkpOT4ePjg+nTp+PChQvw9PTEDz/8AJVKpamYREQkcxortlOnTkFLSwt9+vRBuXLl\n4ObmBkNDQxw9elRtuwMHDsDLywvGxsbQ09ODj48PDh48iOfPn+P+/ftQKpVIT0+HEALa2trQ1dWF\nEEJTMYmISObKamqguLg4GBsbqy2rW7cuoqKi0LFjR2lZRkYG9PX1pdtaWlrIyMhAQkICGjRogNat\nW6Nv377Q1taGtrY2li5dCj09vUJlKem/4WXlK8k5S0NGgDk1jTk1qzTkLMnZ3pfGiu3169dqhQUA\nenp6UCqVasvatWuHNWvWwMbGBkZGRpg3bx60tbWhUqmQmpqK6tWrY/369bCxsUF4eDh8fHywa9cu\n1KhRo8BZoqOjNbJPRa005CwNGQHm1DTm1KzSklMuNFZs+vr6OUpMqVTCwMBAbdngwYPx6tUr9OnT\nBzo6Ohg2bBgiIiJQqVIlbN68GUqlEnZ2dgCAHj16IDQ0FAcOHIC7u3uBs5iYmEBXV/fDd6qIqFQq\nREdHl+icpSEjwJyaxpyaVRpyZmWUE40VW7169bBp0ya1ZXFxcXB2dlZblpiYiIEDB8LPzw8AEBMT\ng4yMDNSpUwcPHjzIcbFJ2bJlUbZs4WLq6uoW+vTlx1AacpaGjABzahpzalZpySkXGrt4xM7ODqmp\nqQgODkZaWhp27NiBpKQkODg4qG0XHh4OX19fvHr1CsnJyQgMDET37t1Rrlw5tGrVCkeOHMHx48eR\nmZmJffv24ebNm2jdurWmYhIRkcxp7Dc2HR0drFq1ClOmTEFQUBDq1KmDZcuWwcDAAJ6enmjSpAm8\nvb3h6emJhIQEtG3bFlpaWnB2dsbYsWMBAK1atcKkSZMQGBiIpKQk1K1bFytWrECtWrU0FZOIiGRO\nY8UGAPXr10dISEiO5atXr5b+W1dXFzNnzsxzDDc3N7i5uWkyFhER/Yfwn9QiIiJZYbEREZGssNiI\niEhWWGxERCQrLDYiIpIVFhsREckKi42IiGSFxUZERLLCYiMiIllhsRERkayw2IiISFZYbEREJCss\nNiIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGxERGRrLDYiIhIVlhsREQkKyw2IiKSFRYbERHJ\nCouNiIhkhcVGRESywmIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYiIpIVjRbbjRs34ObmBisrK7i6\nuuLSpUs5tklPT8e8efPQsmVL2NraYsKECXj16pW0PioqCn379oW1tTXat2+PiIgITUYkIiKZ01ix\nqVQqeHt7o1u3bjh79izc3d0xbNgwpKamqm23bt067N69G+vXr8exY8eQmZmJ8ePHAwBSUlLg5eWF\njh074vz585gxYwbGjRuH+/fvayomERHJnMaK7dSpU9DS0kKfPn1Qrlw5uLm5wdDQEEePHlXb7sCB\nA/Dy8oKxsTH09PTg4+ODgwcP4vnz5zhy5AiMjIzQv39/aGlpoWnTpti+fTsqVaqkqZhERCRzZTU1\nUFxcHIyNjdWW1a1bF1FRUejYsaO0LCMjA/r6+tJtLS0tZGRkICEhAdevX0fdunUxbtw4HDlyBNWr\nV4ePjw8UCkWhsqhUqg/bmSKWla8k5ywNGQHm1DTm1KzSkLMkZ3tfGiu2169fqxUWAOjp6UGpVKot\na9euHdasWQMbGxsYGRlh3rx50NbWhkqlwrNnzxAREYHAwEBMmzYNx44dw8iRIxEeHo46deoUOEt0\ndLRG9qmolYacpSEjwJyaxpyaVVpyyoXGik1fXz9HiSmVShgYGKgtGzx4MF69eoU+ffpAR0cHw4YN\nQ0REBCpVqgQdHR2YmZmhS5cuAID27dvD3Nwcf/75Z6GKzcTEBLq6uh++U0VEpVIhOjq6ROcsDRkB\n5tQ05tSs0pAzK6OcaKzY6tWrh02bNqkti4uLg7Ozs9qyxMREDBw4EH5+fgCAmJgYZGRkoE6dOqhb\nty5OnDihtn1mZiaEEIXKoqurCz09vffYi+JVGnKWhowAc2oac2pWackpFxq7eMTOzg6pqakIDg5G\nWloaduzYgaSkJDg4OKhtFx4eDl9fX7x69QrJyckIDAxE9+7dUa5cOXTs2BGJiYnYsGEDMjMzcejQ\nIVy/fh3t2rXTVEwiIpI5jRWbjo4OVq1ahb1796JZs2bYtGkTli1bBgMDA3h6emL58uUAAE9PT9Ss\nWRNt27aFk5MT6tWrh7FjxwIAatSogY0bN2Lfvn1o2rQp5s6di/nz5+Ozzz7TVEwiIpI5jZ2KBID6\n9esjJCQkx/LVq1dL/62rq4uZM2fmOYa5uXmuYxARERUE/0ktIiKSFRYbERHJCouNiIhkhcVGRESy\nwmIjIiJZYbEREZGssNiIiEhWWGxERCQrLDYiIpIVFhsREckKi42IiGSFxUZERLLCYiMiIllhsRER\nkayw2IiISFZYbEREJCssNiIikhUWGxERyQqLjYiIZIXFRkREssJiIyIiWWGxERGRrLDYiIhIVlhs\nREQkK2WEEOJjh9Ck8+fPf+wIRESljo2NzceOoDGyKzYiIvpv46lIIiKSFRYbERHJCouNiIhkhcVG\nRESywmIjIiJZYbEREZGssNiIiEhWWGxERCQrpbbYfv75Z8yaNSvP9ampqRg/fjyaNWsGe3t7LFu2\nTFonhMDcuXPRvHlzNG3aFD///DMyMjI0mm/9+vVo2bIlGjduDB8fH7x+/TrHNvfv34e1tbXaT8OG\nDdGxY0cp59vrPT09iz0nAFy5cgVmZmZqWZYvXy7lLAnzCQAPHz7E999/D1tbW7Ro0QIBAQFITU2V\ncmp6Pm/cuAE3NzdYWVnB1dUVly5dKnT+PXv2wNHREdbW1hgyZAiSkpI+KNOH5Fy6dCnatGmDJk2a\nwN3dHbdv35bWTZ06FY0aNVKbv/v373+UnJ07d4alpaWUo3PnztK6EydOwNnZGVZWVujTpw/i4uI0\nmrGgOT09PdXmytLSEqamprhw4QKA4plP4M1r18HBIc/1+R1/xTGXRUKUMsnJycLPz08oFAoxc+bM\nPLebOXOmGDBggHj+/LmIi4sTbdu2FYcPHxZCCBEcHCycnZ3Fo0ePRGJioujatavYuHGjxjIeOXJE\nODg4iNjYWPH8+XPh6ekpZsyY8c77JSYmCgcHB3Hs2DEhhBBxcXHCyspKZGZmaizb++bcunWrGDx4\ncK7rStJ89uvXT0ydOlUolUqRmJgoevToIYKCgoQQmp9PpVIpWrZsKTZv3ixSU1PF9u3bRYsWLYRK\npSpw/ps3b4rGjRuLS5cuiZSUFDF+/HgxbNgwjeQrbM7Q0FDRoUMHER8fL9LS0sSSJUtEmzZtREZG\nhhBCiG+//Vbs27dPo9neJ2dKSoowMzMTT548yTHG48ePhbW1tTh8+LBQqVRi0aJFomvXrh8l59vG\njh0rxowZI90u6vnMzMwU27dvFzY2NqJZs2a5bpPf8Vccc1lUSl2xderUSYwfP14MHz4832Kzt7cX\nJ06ckG6vWbNGDBkyRAghhJubm9i+fbu0bv/+/cLZ2VljGUeOHCnmz58v3b569aqwsbER6enp+d5v\n6NCh4ueff5Zu7927V/Tq1UtjuT4k5+TJk6WCeFtJmU+VSiW8vLxEYmKitGzjxo3C3d1dCKH5+fzj\njz9E69at1ZY5OzuL/fv3Fzj/7Nmzha+vr7QuOTlZmJmZiaSkpGLPuXr1ahEWFibdfvHihVAoFOLe\nvXsiIyNDWFlZiTt37mgs1/vmvHz5smjZsmWuY2zevFn06dNHup2eni6aNGkirl69Wuw5szt48KBo\n1aqVePHihRBCFMt8Ll26VLi4uIhVq1blWWz5HX/FMZdFpcSdikxPT8fz589z/Lx8+RLAm1M6gYGB\nMDAwyHOMZ8+eISkpCSYmJtKyunXrIioqCgAQGxubY110dDREIf7ZzPxy5jb+ixcv8OjRozzHO3ny\nJC5cuIBRo0ZJy27evImXL1/C1dUVdnZ2GDFiRL5jFGXOmzdv4sKFC2jXrh3atGmDWbNmSaf4Ssp8\n6ujoYOXKlahWrZq07OjRo6hfv760Dx86n9nFxcXB2NhYbVn24yxLfvnfXmdoaIiKFSsiNjb2vXO9\nb85Bgwaha9eu0u0jR46gcuXK+PTTT3Hnzh0olUrMmjULzZs3R5cuXXD06FGNZSxMzhs3bqBs2bL4\n9ttv0bx5c3h4eCAmJgbAm7nOPoa2tjZq166N6OjoYs+ZJT09HTNmzICfnx8qVKgAAMUyn927d0d4\neDjMzc3z3Ca/46845rKolP3YAd525swZDBw4MMfyzz77DEeOHEGNGjXeOUZKSgoAQF9fX1qmp6cH\npVIprdfT05PW6evrIzMzE6mpqdDV1f3gnNra2jnGz54rNytXroSHhwfKly8vLdPR0YGVlRVGjhwJ\nXV1dBAYGYvjw4di2bVuBMmoyp6GhIWxtbfHtt9/iyZMnGDlyJBYuXAgfH58SOZ9CCAQGBiI2NhZz\n5swBoJn5zO7169dqxxigfpxlyW1+spa/vS5rfX77VlQ5szt79iwmT56MadOmQUtLC8+fP0ezZs3g\n6ekJc3NzHDt2DKNGjcK2bdtgampa7DnNzc3h6+sLIyMjLF26FF5eXoiIiEBKSopUHlk+9nxGRERA\nV1cXnTp1kpYVx3xWr179ndvkd/wVx1wWlRJXbPb29rh169YHjZH1RCmVSumJUSqV0m95enp6UKlU\n0vYpKSkoW7Zsgd+E35XTxcUlx/gA1EoruwcPHuDs2bOYO3eu2vLhw4er3fbz80Pz5s2RmJhYoINW\nkzmzLhQBAAMDAwwZMgRBQUHw8fEpcfOpVCoxduxY3Lp1C8HBwahatSoAzcxndvr6+jnezLIfZ1ly\nm5+s/HkVYX5nJIoqZ5adO3di6tSpmDhxIlxcXAAAVlZW2LBhg7RN+/btYWdnhz/++ENjb8QFzdmr\nVy/06tVLuj169Ghs3rwZN2/ezHWMjz2fYWFh6NmzJ7S0/u8EWXHMZ0Hkd/wVx1wWlRJ3KlITKleu\njKpVq6pdwZP99IGxsXGOdfXq1dPY4xsbG6udSoqLi0PFihXzfPM8evQomjVrhipVqqgtX7lyJa5f\nvy7dzjr1V5jC0ETOZ8+eYdasWdLpYABQqVRSjpI0n0+fPkW/fv3w9OlTbN26FbVr15bWaXo+69Wr\nl+Mqsbi4OLVTO+/K//bcJScn49mzZzlOdX2IguYEgCVLlmDGjBlYunQpunXrJi0/efIkQkJC1LbN\nfgwUZ86tW7fixIkT0u2MjAykp6dDV1c3xxgZGRmIj4/PdV+LOicAvHz5EmfPnsXXX3+ttrw45rMg\n8jv+imMui4osiw0AvvnmGyxatAhPnz7FnTt3sGnTJri6ukrr1qxZg4cPHyIpKQkrVqyQ1mnqsbdu\n3YqoqCi8fPkSCxcuhIuLi9ontuwuX74MKyurHMtjY2Mxc+ZM/Pvvv3jx4gUCAwPh6OiITz75pFhz\nVqxYEQcPHsTixYuRlpaGu3fvYvny5dIbX0mZTyEEhg8fDiMjI6xZswaVK1dWW6/p+bSzs0NqaiqC\ng4ORlpaGHTt2ICkpKcel1fnld3Z2xoEDB3Du3DmoVCoEBQWhVatWMDQ0fK9MH5IzNDQUGzZswJYt\nW2BnZ6e2TktLC7NmzcK5c+eQkZGBPXv24PLlyznesIsjZ2JiIgIDA/HgwQMolUrMnDkT9erVQ/36\n9fHVV1/h2rVrOHDgAFJTU7Fs2TJ8+umnaNCgQbHnBIBr166hevXqOb5CKY75LIj8jr/imMsi83Gv\nXXl/fn5+Oa6KtLKyEmfPnhVCvLkkeOLEiaJ58+bCzs5OLFu2TNouPT1dBAUFiRYtWohmzZqJgICA\nd16xWFgbNmwQbdu2FTY2NmLMmDHi9evXQggh7t27J6ysrMS9e/ekbfv27Su2bNmSY4wXL14If39/\nYWtrKxo3bizGjBkjnj59+lFyRkVFiQEDBojGjRsLe3t7sWDBAumy+ZIyn+fPnxcKhUKYm5sLKysr\n6Sfryq6imM+bN2+Kb7/9VlhZWQlXV1dx8eJFIYQQgwYNUjvm8sovxJurNTt06CCsra2Fl5eXRq+I\nLEzODh06iAYNGqjNnZWVlYiOjhZCCLFt2zbx1VdfCUtLS9GlSxdx+vTpj5IzNTVVTJ8+XbRo0UJY\nWVkJLy8vtdfTyZMnhYuLi7CyshK9e/cWsbGxHyWnEELs2LFD9OjRI9cximM+hRDi1KlTaldFTpw4\nUUycOFG6nd/xVxxzWRT4f9AmIiJZke2pSCIi+m9isRERkayw2IiISFZYbEREJCssNiIikhUWGxER\nyQqLjYiIZIXFRkREsvL/ALQhEV1a0SErAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c261c5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain feature importances\n",
    "pca_var_explained = pca.explained_variance_ratio_ \n",
    "feature_labels = df_label\n",
    "\n",
    "# cumulative sum of explained ration\n",
    "cumulative_var_explained = np.cumsum(pca_var_explained)\n",
    "\n",
    "# cumulative distribution function for principle componets\n",
    "# based on explained variance ratio\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.set_context('talk')\n",
    "\n",
    "plt.plot(range(df_label.shape[1]), cumulative_var_explained)\n",
    "plt.xlim([-1, df_label.shape[1]])\n",
    "plt.title('Principal Components:  Cumulative Explained Variance Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring feature extraction with Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(solver='eigen',n_components=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using numeric and categorical data w/ dummy variables\n",
    "X_train_for_lda = X_train[:,[0,1,7,9,12,15,16,17,18,19,20,21,22,23,24,25,26,27]]\n",
    "y_train_for_lda = y_train\n",
    "\n",
    "# standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_for_lda_std = stdsc.fit_transform(X_train_for_lda)\n",
    "\n",
    "# fitting pca to the data\n",
    "X_train_lda = lda.fit_transform(X_train_for_lda_std,y_train_for_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain feature importances\n",
    "lda_var_explained = lda.explained_variance_ratio_ \n",
    "\n",
    "# cumulative sum of explained ration\n",
    "lda_cumulative_var_explained = np.cumsum(lda_var_explained)\n",
    "\n",
    "# cumulative distribution function for principle componets\n",
    "# based on explained variance ratio\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.set_context('talk')\n",
    "\n",
    "plt.plot(range(1,3), lda_cumulative_var_explained)\n",
    "plt.xlim(0,3)\n",
    "plt.title('Linear Discriminants:  Cumulative Explained Variance Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Machine Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "# prints initial weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "max_steps = 1000\n",
    "\n",
    "#dataset splitting - test and train\n",
    "X = df.iloc[:,0:500]\n",
    "y = df.iloc[:,500]\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size = 0.55, random_state = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>272.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>4468.0</td>\n",
       "      <td>6476.0</td>\n",
       "      <td>7948.0</td>\n",
       "      <td>7162.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>2608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>978.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>4668.0</td>\n",
       "      <td>6678.0</td>\n",
       "      <td>8310.0</td>\n",
       "      <td>7560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>146.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>17278.0</td>\n",
       "      <td>36176.0</td>\n",
       "      <td>71220.0</td>\n",
       "      <td>155602.0</td>\n",
       "      <td>272204.0</td>\n",
       "      <td>366718.0</td>\n",
       "      <td>363102.0</td>\n",
       "      <td>281158.0</td>\n",
       "      <td>178122.0</td>\n",
       "      <td>103624.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75998.0</td>\n",
       "      <td>40990.0</td>\n",
       "      <td>16642.0</td>\n",
       "      <td>33504.0</td>\n",
       "      <td>73508.0</td>\n",
       "      <td>189156.0</td>\n",
       "      <td>372634.0</td>\n",
       "      <td>552648.0</td>\n",
       "      <td>564664.0</td>\n",
       "      <td>438580.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>224.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>718.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>1096.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>120.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>632.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1        2         3         4         5         6    \\\n",
       "31      272.0    164.0    634.0    2120.0    4468.0    6476.0    7948.0   \n",
       "1052    146.0     32.0     22.0      26.0     164.0     158.0     172.0   \n",
       "1936  17278.0  36176.0  71220.0  155602.0  272204.0  366718.0  363102.0   \n",
       "841     224.0    290.0    278.0     140.0     112.0      98.0      44.0   \n",
       "545     120.0    152.0    148.0     246.0     480.0     342.0     290.0   \n",
       "\n",
       "           7         8         9      ...         490      491      492  \\\n",
       "31      7162.0    5400.0    2608.0    ...       978.0    468.0     70.0   \n",
       "1052     168.0     234.0     230.0    ...       140.0      0.0    274.0   \n",
       "1936  281158.0  178122.0  103624.0    ...     75998.0  40990.0  16642.0   \n",
       "841       38.0       0.0       0.0    ...       718.0    334.0    418.0   \n",
       "545      466.0     664.0     516.0    ...       128.0     20.0    138.0   \n",
       "\n",
       "          493      494       495       496       497       498       499  \n",
       "31      126.0    794.0    2268.0    4668.0    6678.0    8310.0    7560.0  \n",
       "1052    114.0     38.0     104.0     110.0     150.0     112.0     136.0  \n",
       "1936  33504.0  73508.0  189156.0  372634.0  552648.0  564664.0  438580.0  \n",
       "841     572.0    906.0    1096.0    1260.0    1542.0    2002.0    1618.0  \n",
       "545     200.0    424.0     678.0     904.0     952.0     672.0     632.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31        5446.0\n",
       "1052       116.0\n",
       "1936    255458.0\n",
       "841       1098.0\n",
       "545        352.0\n",
       "Name: 500, dtype: float32"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 200 input samples and 1 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-171005aed6df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           callbacks=[history])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1436\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m-> 1438\u001b[0;31m         \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1440\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 200 input samples and 1 target samples."
     ]
    }
   ],
   "source": [
    "history = TrainingHistory()\n",
    "bbs = np.array(x, ndmin=2).T\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          epochs=2000,\n",
    "          verbose=0,\n",
    "          batch_size=50,\n",
    "          callbacks=[history])\n",
    "\n",
    "# print trained weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "'neural net weights after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(bbs)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates the animation\n",
    "import matplotlib.animation as animation\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "def update_line(num):\n",
    "    line.set_xdata(x)\n",
    "    line.set_ydata(history.predictions[num])\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_line, init_func=init, frames=len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "\n",
    "ani.save('neuron_training.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using single layer to train \n",
    "\n",
    "# input\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# inference\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([1, 10]))\n",
    "matm=tf.matmul(x,W)\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# loss\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# training\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# training cycles\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(10) # mini-batch\n",
    "    \n",
    "    _, y_pred = sess.run((train_step, y), feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    correct_prediction = tf.equal(tf.argmax(y,axis=1), tf.argmax(y_,axis=1)) # y=(m*c) so axis=1 along c\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (sess.run(correct_prediction, feed_dict={x: batch_xs, y_: batch_ys}))\n",
    "    print (sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys}))\n",
    "\n",
    "    \n",
    "# Validation (using test set to evaluate the accuracy: there are 10000 images in it\n",
    "\n",
    "print (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Weight Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8767\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "masks = []\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(30)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    masks.append(sess.run(W)) # save the intermediate weights\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e1f6e9c99e45fdb602cd6ad2d8be51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.view_mask>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def view_mask(number, learning_iteration):\n",
    "    data = np.transpose(masks[learning_iteration])[number]\n",
    "    pos = map(lambda x: x if x > 0 else 0, data)\n",
    "    neg = map(lambda x: x if x < 0 else 0, data)\n",
    "    pos /= np.amax(pos)\n",
    "    neg /= np.amin(neg)\n",
    "    data = pos - neg\n",
    "    image = np.split(data, 28)\n",
    "    plt.imshow(image);\n",
    "\n",
    "interact(view_mask, number=(0,9), learning_iteration=(0, len(masks) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
